name: "DeepDeinterlacing"
input: "input"
input_dim: 64
input_dim: 3
input_dim: 30
input_dim: 30

input: "label"
input_dim: 64
input_dim: 1
input_dim: 30
input_dim: 30

input: "interlace"
input_dim: 64
input_dim: 1
input_dim: 30
input_dim: 30

input: "deinterlace"
input_dim: 64
input_dim: 1
input_dim: 30
input_dim: 30

input: "inv-mask"
input_dim: 64
input_dim: 1
input_dim: 30
input_dim: 30


## conv1
layer {  name: "conv1_1"  type: "Convolution"  bottom: "input"  top: "conv1_1"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 16    kernel_size: 3    stride: 1    pad: 1
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu1_1"  type: "ReLU"  bottom: "conv1_1"  top: "conv1_1" }

layer {  name: "conv1_2"  type: "Convolution"  bottom: "conv1_1"  top: "conv1_2"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 16    kernel_size: 3    stride: 1    pad: 1
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu1_2"  type: "ReLU"  bottom: "conv1_2"  top: "conv1_2" }

## conv2
layer {  name: "conv2_1"  type: "Convolution"  bottom: "conv1_2"  top: "conv2_1"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 32    kernel_size: 3    stride: 1    pad: 1
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu2_1"  type: "ReLU"  bottom: "conv2_1"  top: "conv2_1" }

layer {  name: "conv2_2"  type: "Convolution"  bottom: "conv2_1"  top: "conv2_2"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 32    kernel_size: 3    stride: 1    pad: 1
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu2_2"  type: "ReLU"  bottom: "conv2_2"  top: "conv2_2" }

## conv3
layer {  name: "conv3_1"  type: "Convolution"  bottom: "conv2_2"  top: "conv3_1"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 64    kernel_size: 3    stride: 1    pad: 1
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu3_1"  type: "ReLU"  bottom: "conv3_1"  top: "conv3_1" }

layer {  name: "conv3_2"  type: "Convolution"  bottom: "conv3_1"  top: "conv3_2"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 64    kernel_size: 3    stride: 1    pad: 1
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu3_2"  type: "ReLU"  bottom: "conv3_2"  top: "conv3_2" }

layer {  name: "conv3_3"  type: "Convolution"  bottom: "conv3_2"  top: "conv3_3"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 64    kernel_size: 3    stride: 1    pad: 1
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu3_3"  type: "ReLU"  bottom: "conv3_3"  top: "conv3_3" }

############## DSN ##############
layer {
  name: "conv-dsn"  type: "Convolution"  bottom: "conv3_3"  top: "conv-dsn"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 1    kernel_size: 1    stride: 1    pad: 0
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}

# residual learning
layer {
  name: "eltwise-dsn-residual"
  type: "Eltwise"
  bottom: "conv-dsn"
  bottom: "deinterlace"
  top: "output-dsn-residual"
  eltwise_param { operation: SUM }
}

#inverse mask
layer {
  name: "eltwise-dsn-mask"
  type: "Eltwise"
  bottom: "output-dsn-residual"
  bottom: "inv-mask"
  top: "output-dsn-inv-mask"
  eltwise_param { operation: PROD }
}

#combine origin image
layer {
  name: "eltwise-dsn-combine"
  type: "Eltwise"
  bottom: "output-dsn-inv-mask"
  bottom: "interlace"
  top: "output-combine"
  eltwise_param { operation: SUM }
}

## loss
layer { name: "loss-dsn"  type: "EuclideanLoss"  bottom: "output-combine"  bottom: "label"  top: "loss-dsn" loss_weight: 1 }