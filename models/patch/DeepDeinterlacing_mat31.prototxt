name: "DeepDeinterlacing"
input: "input"
input_dim: 64
input_dim: 1
input_dim: 16
input_dim: 33

input: "label"
input_dim: 64
input_dim: 1
input_dim: 16
input_dim: 8

input: "deinterlace"
input_dim: 64
input_dim: 1
input_dim: 16
input_dim: 8


## conv1
layer {  name: "conv1_1"  type: "Convolution"  bottom: "input"  top: "conv1_1"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 64
	kernel_w: 5    stride_w: 1    pad_w: 2
	kernel_h: 5    stride_h: 1    pad_h: 2
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu1_1"  type: "ReLU"  bottom: "conv1_1"  top: "conv1_1" }

layer {  name: "conv1_2"  type: "Convolution"  bottom: "conv1_1"  top: "conv1_2"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 32
	kernel_w: 5    stride_w: 1    pad_w: 2
	kernel_h: 5    stride_h: 1    pad_h: 2
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu1_2"  type: "ReLU"  bottom: "conv1_2"  top: "conv1_2" }

layer {  name: "conv1_3"  type: "Convolution"  bottom: "conv1_2"  top: "conv1_3"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 16
	kernel_w: 5    stride_w: 4    pad_w: 0
	kernel_h: 5    stride_h: 1    pad_h: 2
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}
layer {  name: "relu1_3"  type: "ReLU"  bottom: "conv1_3"  top: "conv1_3" }

############## Output ##############
layer {
  name: "conv-dsn"  type: "Convolution"  bottom: "conv1_3"  top: "conv-dsn"
  param {    lr_mult: 1  }
  param {    lr_mult: 0.1  }
  convolution_param {
    num_output: 1    kernel_size: 1    stride: 1    pad: 0
    weight_filler { type: "msra" }
    bias_filler {      type: "constant"      value: 0    }
  }
}

# residual learning
layer {
  name: "eltwise-dsn-residual"
  type: "Eltwise"
  bottom: "conv-dsn"
  bottom: "deinterlace"
  top: "output-combine"
  eltwise_param { operation: SUM }
}

## loss
layer { name: "loss-dsn"  type: "EuclideanLoss"  bottom: "output-combine"  bottom: "label"  top: "loss-dsn" loss_weight: 1 }